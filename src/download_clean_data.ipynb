{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_URL = \"http://127.0.0.1:5000/inference\"\n",
    "DATA_PATH = \"../data/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(img: np.array):\n",
    "    resp = requests.post(\n",
    "        APP_URL, \n",
    "        json={\"image\": img.tolist()},\n",
    "    )\n",
    "    return np.array(\n",
    "        resp.json()[\"prediction\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def store_local(img, column, basename, base_dir='C:/Users/NPDan/Documents/GitHub/task_challenge/data',\n",
    "                image_stem='images', mask_stem = 'masks', ftype='png'):\n",
    "    \"\"\"helper to store local image files for ease\"\"\"\n",
    "    # determine image path\n",
    "    dst_dir = Path(base_dir) / (image_stem if image_stem[:-1] in column else mask_stem)\n",
    "    dst_path = dst_dir / '.'.join((basename, ftype))\n",
    "    \n",
    "    # verify directory and save\n",
    "    if not dst_dir.is_dir():\n",
    "        dst_dir.mkdir()\n",
    "    img.save(str(dst_path))\n",
    "    return str(dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "download = True\n",
    "urls = pd.read_csv(\"../data/data.csv\")\n",
    "\n",
    "# set up new df and cols\n",
    "base_dir = Path('C:/Users/NPDan/Documents/GitHub/task_challenge/data')\n",
    "json_path = base_dir / 'data_local.json'\n",
    "local_cols = ['local_image','local_mask']\n",
    "\n",
    "if download:\n",
    "    # copy df, add cols\n",
    "    urls_local = urls.copy()\n",
    "    urls_local[local_cols] = None, None\n",
    "\n",
    "    # loop over \n",
    "    for ix, row in urls.iterrows():\n",
    "        for ic, col in enumerate([\"image_url\", \"mask_url\"]):\n",
    "            # get response, load data\n",
    "            resp = requests.get(row[col])\n",
    "            img = Image.open(BytesIO(resp.content))\n",
    "\n",
    "            # write local image, retrieve path\n",
    "            basename = f'img_{ix:03}'\n",
    "            dst_path = store_local(img, col, basename)\n",
    "\n",
    "            # store path in updated df\n",
    "            urls_local.iloc[ix, ic + len(row)] = dst_path\n",
    "\n",
    "    # save locals to json\n",
    "    urls_local.to_json(json_path)\n",
    "else:\n",
    "    urls_local = pd.read_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mask_url</th>\n",
       "      <th>image_url</th>\n",
       "      <th>local_image</th>\n",
       "      <th>local_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/ab53069c...</td>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/3e18e6da...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/8728de7a...</td>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/5daefb3e...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/baa75595...</td>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/74217c21...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/2e1508f0...</td>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/eeb2ed67...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/2b42cbd4...</td>\n",
       "      <td>https://d1h90vpqo1860x.cloudfront.net/ba7e2239...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "      <td>C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            mask_url  \\\n",
       "0  https://d1h90vpqo1860x.cloudfront.net/ab53069c...   \n",
       "1  https://d1h90vpqo1860x.cloudfront.net/8728de7a...   \n",
       "2  https://d1h90vpqo1860x.cloudfront.net/baa75595...   \n",
       "3  https://d1h90vpqo1860x.cloudfront.net/2e1508f0...   \n",
       "4  https://d1h90vpqo1860x.cloudfront.net/2b42cbd4...   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  https://d1h90vpqo1860x.cloudfront.net/3e18e6da...   \n",
       "1  https://d1h90vpqo1860x.cloudfront.net/5daefb3e...   \n",
       "2  https://d1h90vpqo1860x.cloudfront.net/74217c21...   \n",
       "3  https://d1h90vpqo1860x.cloudfront.net/eeb2ed67...   \n",
       "4  https://d1h90vpqo1860x.cloudfront.net/ba7e2239...   \n",
       "\n",
       "                                         local_image  \\\n",
       "0  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...   \n",
       "1  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...   \n",
       "2  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...   \n",
       "3  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...   \n",
       "4  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...   \n",
       "\n",
       "                                          local_mask  \n",
       "0  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...  \n",
       "1  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...  \n",
       "2  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...  \n",
       "3  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...  \n",
       "4  C:\\Users\\NPDan\\Documents\\GitHub\\task_challenge...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_local.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to be sure, let's verify all files are there and accessible\n",
    "for ix, row in urls_local.iterrows():\n",
    "    for col in local_cols:\n",
    "        fpath = Path(row[col])\n",
    "        if not fpath.is_file():\n",
    "            print(f'File not found' + ' - '.join(fpath.parts[-2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updates and data pruning\n",
    "Data access looks good. Having looked at the images and masks, I noted a few observations:\n",
    "1. Data to prune - There were 2 obvious instances where the segmented building was neither the central building nor entirely in frame. That seemed like a poor sample, so I tracked those two *image* names and we can purge them.\n",
    "2. Contrast and brightness can vary significantly even within the same image due to shadows from low-sun angles and adjacent trees/structures. This seems like a good case for contrast / brightness / noise augmention\n",
    "3. There's a variety of orientations within the dataset - some houses oblong rectangles and others very square, oriented in various angles (not just horizontal or vertical). Good argument to use some image rotation augmentation\n",
    "4. Crop / Resize - There's not much variance in the scale of the images and many objects are close to the border of the image. I'm not sure that Cropping / Resizing is necessary, perhaps we can explore two different augmentation strategies and compare, but we might be fine with skipping this.\n",
    "\n",
    "So, next steps...\n",
    "Let's copy and update the DF to exclude these two bad image examples and store that result as the **clean** dataset. This is what we'll use to generate training, testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_img_names = ['img_025.png', 'img_144.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 240)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = 'images'\n",
    "clean_urls_local = urls_local.copy()\n",
    "for bad_img in bad_img_names:\n",
    "    bad_img_path = (base_dir / image_dir) / bad_img\n",
    "    clean_urls_local.loc[clean_urls_local.local_image == str(bad_img_path), 'local_image'] = pd.NA\n",
    "clean_urls_local.dropna(0, 'any', inplace=True)\n",
    "len(clean_urls_local), len(urls_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks good, let's store the result\n",
    "json_path = base_dir / 'data_local_clean.json'\n",
    "clean_urls_local.to_json(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, data exploration and pruning is done. Next, let's build a dataloader pipeline in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pep8 reminder\n",
    "def foo(val:int=1, bar:bool=None) -> None:\n",
    "    print('foo' * val)\n",
    "    if bar:\n",
    "        print('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foofoo\n",
      "bar\n"
     ]
    }
   ],
   "source": [
    "foo(2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
